<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator>
  <link href="/tag/applications/feed.xml" rel="self" type="application/atom+xml" />
  <link href="/" rel="alternate" type="text/html" />
  <updated>2024-02-12T17:39:21+00:00</updated>
  <id>/tag/applications/feed.xml</id>

  
  
  

  
    <title type="html">Exploring AI | </title>
  

  
    <subtitle>Come explore the wonders of AI</subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">Machine Learning in Chemistry</title>
      <link href="/chemistry-ml" rel="alternate" type="text/html" title="Machine Learning in Chemistry" />
      <published>2024-02-09T08:00:00+00:00</published>
      <updated>2024-02-09T08:00:00+00:00</updated>
      <id>/chemistry-ml</id>
      <content type="html" xml:base="/chemistry-ml">&lt;h1 id=&quot;machine-learning-in-chemistry&quot;&gt;Machine Learning in Chemistry&lt;/h1&gt;

&lt;p&gt;Machine learning is becoming a significant tool in the field of chemistry, providing new opportunities in various areas such as drug discovery and materials science. Machine learning algorithms, especially neural networks, are effective at identifying complex patterns in chemical data, which can lead to new insights and speed up processes that were previously dependent on traditional, more time consuming methods. As we examine the impact of machine learning on chemistry, we will look at its uses, and how it not only simplifies regular tasks but also leads to advancements in understanding molecular complexities.&lt;/p&gt;

&lt;p&gt;Recently, the combination of machine learning and chemistry has made significant progress. Researchers are using advanced models like CNNs and RNNs for tasks such as creating new drugs, predicting toxicology, and modeling quantitative structure-activity relationships. The pursuit of models that are interpretable and explainable is becoming more important, giving scientists a better understanding of why predictions are made. Additionally, the use of multi-modal data and the development of transfer learning techniques are expanding what can be achieved in predicting material properties and optimizing synthesis planning. These recent trends highlight the growing collaboration between machine learning and chemistry, pushing scientific research into new areas and influencing the future of chemical research.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;chemistry-applications-for-machine-learning&quot;&gt;Chemistry Applications for Machine Learning&lt;/h2&gt;

&lt;p&gt;A lot of different applications in chemistry research are utilizing machine learning to accelerate processes, discover new chemicals and compounds, and predict behaviors of chemicals in different environments and reactions. Let’s take a look at some interesting chemistry domains, and see how machine learning is being leveraged to further research.&lt;/p&gt;

&lt;h4 id=&quot;drug-discovery-and-design&quot;&gt;Drug Discovery and Design&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/drug.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Machine learning can aid in the discovery of new drugs by making predictions at different points in the process. For drugs to take effect on the body, they need to bind to a protein or enzyme in order to trigger the cascade of reactions that lead to the desired effect.  Predicting the drug-protein interaction (DPI) is crucial for drug discovery, since accurate predictions can serve as strong indicators for the biological function that will be affected. The efficacy of a drug, prognostic biomarker identification, clinical trial design, and even FDA approval can also be predicted with machine learning. One of the fields that is really seeing a big focus on drug discovery with machine learning is oncology research, since ML assisted development can rapidly accelerate the discovery of therapeutics to help those that aren’t benefiting from existing treatments.&lt;/p&gt;

&lt;p&gt;Here are a few interesting resources related to drug discovery:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10014302/&quot;&gt;Machine learning approaches to predict drug efficacy and toxicity in oncology&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7577280/pdf/main.pdf&quot;&gt;Artificial intelligence in drug discovery and development&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.mdanderson.org/newsroom/md-anderson-generate-biomedicines-co-develop-protein-therapies-cancer-using-generative-ai.h00-159617856.html&quot;&gt;MD Anderson and Generate:Biomedicines enter co-development and commercialization agreement to accelerate novel protein therapeutics for oncology using generative AI&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10302550/pdf/main.pdf&quot;&gt;AI in drug discovery and its clinical relevance&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-1052-0&quot;&gt;A deep learning-based method for drug-target interaction prediction based on long short-term memory neural network&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;retrosynthesis&quot;&gt;Retrosynthesis&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/molecule-dark.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In organic chemistry, retrosynthesis is the process of breaking down an organic molecule into its smaller precursor compounds (usually a compound that is simple or commercially available), then working backward from those precursors to synthesize the original target molecule. The goal is not only to find a viable synthetic pathway, but also to find an efficient one, since more than one synthetic route will be possible. &lt;sub&gt;[4]&lt;/sub&gt; Machine learning can augment this research process by predicting reaction pathways&lt;sub&gt;[19] [21]&lt;/sub&gt;, recommending reaction conditions&lt;sub&gt;[20]&lt;/sub&gt;, and predicting likelihood of success for a particular reaction.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.microsoft.com/en-us/research/blog/incorporating-chemists-insight-with-ai-models-for-single-step-retrosynthesis-prediction/&quot;&gt;Microsoft Research Blog: Incorporating chemists’ insight with AI models for single-step retrosynthesis prediction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://academic.oup.com/bib/article/23/1/bbab391/6375056&quot;&gt;Deep learning in retrosynthesis planning: datasets, models and tools&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/2301.05864&quot;&gt;Recent advances in artificial intelligence for retrosynthesis&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;atomic-simulations&quot;&gt;Atomic Simulations&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/atom-sim.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In computational chemistry, systems of atoms or molecules can be simulated to study the behavior of these molecules under certain conditions, which is very useful for predicting material properties. One category of atomic simulations is molecular dynamics, where the physical motion of molecules are modeled using Newtonian mechanics and force fields. Another type of atomic simulation is one that uses the electronic structure and behavior of atoms using quantum mechanics.&lt;/p&gt;

&lt;p&gt;In many cases (especially with quantum chemistry simulations), calculations used for simulating molecular systems are very expensive to compute, sometimes taking weeks to complete. This high computational cost will limit the size of the system being modeled (number of molecules), or the duration of the simulation. Researchers are using machine learning to increase the calculation speed, allowing for larger system simulations.&lt;sub&gt;[23]&lt;/sub&gt; In the context of systems with quantum mechanics calculations, machine learning can be used to predict potential energy surfaces rather than having to solve the Schrödinger equation, allowing faster calculations for large systems. &lt;sub&gt;[24]&lt;/sub&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://youtube.com/shorts/AMhdgg26Bg0?si=FfRTr20ZnLQRY2Vb&quot;&gt;44 Million Atoms Simulated Using AI&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.nature.com/articles/s41524-022-00959-5&quot;&gt;Machine-learning atomic simulation for heterogeneous catalysis&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.synopsys.com/glossary/what-are-atomistic-simulations.html&quot;&gt;What are Atomistic Simulations?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;catalysis&quot;&gt;Catalysis&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/catalysis.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Catalysis is the process where a substance, known as a catalyst, increases the rate of a chemical reaction without being consumed or chemically changed at the end of the reaction. There are several different classifications of catalysis, each with unique characteristics and applications:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Homogeneous Catalysis: catalyst and the reactants are in the same phase of matter, usually gas or liquid&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Heterogeneous Catalysis: the catalyst is in a different phase from the reactants. Typical examples involve a solid catalyst with the reactants as either liquids or gases &lt;sub&gt;[27]&lt;/sub&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Autocatalysis: this is a type of catalysis where one of the products of the reaction acts as a catalyst for the reaction &lt;sub&gt;[29]&lt;/sub&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Enzyme Catalysis: this involves enzymes, which are biological catalysts, that speed up biochemical reactions &lt;sub&gt;[28]&lt;/sub&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Photocatalysis: light is used to speed up a reaction &lt;sub&gt;[30]&lt;/sub&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Environmental Catalysis and Green Catalytic Processes: these are types of catalysis aimed at reducing environmental impact and promoting sustainability &lt;sub&gt;[26]&lt;/sub&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Machine learning provides a promising avenue for advancing catalysis research by enabling  reaction mechanism prediction&lt;sub&gt;[25]&lt;/sub&gt;, network exploration, synthetic condition optimization, and catalyst design, while also fostering interpretability and integration with experimental data.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.nature.com/collections/gfbfaeaibd&quot;&gt;Nature - Machine learning in catalysis&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pubs.acs.org/doi/pdf/10.1021/acscatal.3c03417&quot;&gt;Accelerating Biocatalysis Discovery with Machine Learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.nature.com/articles/s41929-023-00933-4&quot;&gt;Rates Against the Machine&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;other-applications&quot;&gt;Other Applications&lt;/h4&gt;

&lt;p&gt;Obviously chemistry is a huge field, but the broad applicability of ML is making it so it finds its way into all branches of chemistry research. A few other interesting applications include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Predictive toxicology - large datasets from sources like high-throughput assays&lt;sub&gt;[31]&lt;/sub&gt; and omics&lt;sub&gt;[32]&lt;/sub&gt; approaches to predict the toxicity of chemicals&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Spectroscopy and analytical chemistry - machine learning can be used to automate the identification of compounds in complex spectral data&lt;sub&gt;[33] [34]&lt;/sub&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Materials Science - ML techniques have been used to accelerate and improve the accuracy of predictions of phase diagrams, crystal structures, and other material properties &lt;sub&gt;[35]&lt;/sub&gt;&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/machine-learning-in-materials-science-8c6c0db5ce7a&quot;&gt;An article I wrote on ML in Materials Science&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;ml-algorithms-and-use-cases-in-chemistry&quot;&gt;ML Algorithms and Use-Cases in Chemistry&lt;/h2&gt;

&lt;p&gt;Now that we’ve looked through the “chemistry lens” and seen how research in the field can be advanced by leveraging machine learning, let’s shift perspective, and look through the “machine learning lens” by focusing on how different ML algorithms are being used in chemistry research.&lt;/p&gt;

&lt;h4 id=&quot;decision-trees&quot;&gt;Decision Trees&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/tree.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Decision Trees are popular with researchers in the natural sciences like chemistry due to the interpretability of the model predictions, allowing for transferability to other problems in the field. On top of that, decision trees work well with smaller datasets. This often makes this algorithm a natural fit, since a common obstacle for machine learning in chemistry research is the lack of availability of high-quality data.&lt;sub&gt;[2]&lt;/sub&gt; Finally, fewer hyperparameters to tune and lower computational costs never hurt. A few drawbacks with decision trees are that they don’t always play nice with noisy data or large datasets, and the limitation of decision nodes to binary outcomes can leave some wanting a little more flexibility.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;https://scikit-learn.org/stable/modules/tree.html&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;https://www.coursera.org/articles/decision-tree-machine-learning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;https://www.ibm.com/topics/decision-trees&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;convolutional-neural-networks&quot;&gt;Convolutional Neural Networks&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/cnnet.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Convolutional neural networks are best suited for working with visual data and tasks like image classification, recognition, and segmentation. The real power of CNNs is their effectiveness in tasks where spatial arrangement of features and local patterns are important for accurate analysis. In chemistry, these strengths can be utilized for tasks like gas leak detection, prediction of molecular properties&lt;sub&gt;[7]&lt;/sub&gt; with SMILES,&lt;sub&gt;[8]&lt;/sub&gt; prediction of materials properties, and protein structure prediction as with AlphaFold1. &lt;sub&gt;[10]&lt;/sub&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/towards-data-science/intro-to-pytorch-2-convolutional-neural-networks-487d8a35139a&quot;&gt;An Intro to PyTorch I wrote with a short section on CNNs&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;&quot;&gt;https://cs231n.github.io/convolutional-networks/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week8/convolutional_neural_networks/?activate_block_id=block-v1%3AMITx%2B6.036%2B1T2019%2Btype%40sequential%2Bblock%40convolutional_neural_networks&quot;&gt;MIT Open Library CNN notes&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week8/convolutional_neural_networks/?activate_block_id=block-v1%3AMITx%2B6.036%2B1T2019%2Btype%40sequential%2Bblock%40convolutional_neural_networks&quot;&gt;MIT Introduction to Deep Learning 2022 lecture on CNNs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;recurrent-neural-networks&quot;&gt;Recurrent Neural Networks&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/rnnet.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Recurrent Neural Networks (RNNs) are designed for processing sequential data, making them well-suited for tasks where temporal dependencies and the order of input elements matter. RNNs excel in natural language processing, time series analysis, and speech recognition, as they can capture and leverage information from previous steps in the sequence.&lt;/p&gt;

&lt;p&gt;One of the cool ways RNNs are used in chemistry research are generating de novo molecular designs with SMILES string representations of chemical structures.&lt;sub&gt;[11]&lt;/sub&gt; SMILES characters are the tokens for the model, and the sequence is generated one token at a time, given the previous section of the string. RNNs have also been used in Quantitative Structure-Activity Relationship (QSAR)&lt;sub&gt;[13]&lt;/sub&gt; modeling in organic material discovery&lt;sub&gt;[9]&lt;/sub&gt;, where the activity of a molecule is predicted using chemical/physical properties and molecular descriptors as the features. Another interesting use of RNN’s is the discovery of novel chemical reactions with generative RNNs, specifically a sequence-to-sequence autoencoder with bidirectional LSTM layers.&lt;sub&gt;[12]&lt;/sub&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks&quot;&gt;Stanford RNN Cheat Sheet&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://aws.amazon.com/what-is/recurrent-neural-network/&quot;&gt;AWS: What is an RNN?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.RNN.html&quot;&gt;PyTorch RNN Docs&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;graph-neural-networks&quot;&gt;Graph Neural Networks&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/graph.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Graph Neural Networks are based on the mathematical structure of graphs, consisting of nodes and edges that represent a relationship between two nodes. GNNs are trained on &lt;em&gt;data that is represented as graphs&lt;/em&gt;, and typically (although not always) work via message passing between nodes. &lt;sub&gt;[14]&lt;/sub&gt; Typical applications for graphs and GNNs include social networks, recommendation systems, and network traffic. In chemistry, GNNs are excellent for modeling molecules since they can naturally be represented by graphs. GNNs have been used to predict molecular and crystal properties, as well as modeling potential-energy surfaces for discovery of new materials with desired properties. &lt;sub&gt;[15]&lt;/sub&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://blogs.nvidia.com/blog/what-are-graph-neural-networks/&quot;&gt;NVIDIA: What are Graph Neural Networks?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch-geometric.readthedocs.io/en/latest/&quot;&gt;PyTorch Geometric&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/06-graph-neural-networks.html&quot;&gt;PyTorch Lightning GNN Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;transformers&quot;&gt;Transformers&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/transformer.jpeg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Transformers are neural networks that learn context via a mathematical technique called &lt;strong&gt;attention&lt;/strong&gt; to learn relationships in sequential data such as text. Transformers are considered &lt;em&gt;foundation models&lt;/em&gt;, meaning they can be trained on broad data, and later fine-tuned to more specific downstream tasks. &lt;sub&gt;[16]&lt;/sub&gt; Popular transformer models include GPT, BERT, and T5. AlphaFold2, developed by Google DeepMind is probably the most notable application of transformer models in the chemistry space, predicting the 3-dimensional conformation of proteins, which previously had proven to be a very difficult challenge for researchers. &lt;sub&gt;[17]&lt;/sub&gt; Like many other types of models, transformers are also used in drug discovery, retrosynthetic planning, and exploration of chemical space. &lt;sub&gt;[18]&lt;/sub&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://blogs.nvidia.com/blog/what-is-a-transformer-model/&quot;&gt;NVIDIA: What is a Transformer Model?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html&quot;&gt;PyTorch Transformer Documentation&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Attention is All You Need&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;important-considerations-and-challenges-in-the-field&quot;&gt;Important Considerations and Challenges in the Field&lt;/h2&gt;

&lt;p&gt;Although applying machine learning to chemistry research holds a ton of promise, there are still some hurdles to overcome in the field.  High quality, and sufficiently large datasets are lacking in the chemistry field, and it’s quite common for data to be scattered across multiple databases, stemming from multiple sources. Stitching data together from multiple sources also brings the issue of lack of standardization for data formats, representation, and evaluation metrics.&lt;/p&gt;

&lt;p&gt;Additionally, with expertise in the domain of chemistry, researchers may sometimes be unfamiliar with state-of-the-art machine learning methods, computer science techniques, and best practices. This can make it difficult to effectively identify and engineer important features that can best represent complex chemical systems that may not be straightforward for traditional ML algorithms. Finally, since chemical systems can be so complex and tricky to represent, different types of neural network models are commonly used since they can capture complex data relationships. This, however, comes with the caveat that they lack interpretability that something like a decision tree would provide. Furthermore, chemistry datasets typically exhibit a bias towards successful experiments&lt;sub&gt;[36]&lt;/sub&gt;. Including data from failed experiments as well as successful ones will allow for a more comprehensive understanding and analysis of the chemical space.&lt;/p&gt;

&lt;p&gt;The good news is that recent breakthroughs in LLMs are sparking a ton of interest in machine learning, and this is attracting interest from people with all sorts of backgrounds.  Hopefully with this surge of interest, many will feel encouraged to develop their programming, data, and machine learning skills.  The development of new ML algorithms, data collection strategies, and collaboration between chemists and data scientists will be key to overcoming these challenges and realizing the full potential of ML in chemistry.&lt;/p&gt;

&lt;p&gt;I hope you enjoyed reading! If you enjoyed or found this content helpful, consider subscribing or following to receive updates on future posts. Feel free to make suggestions for content you’d like to read in the future in the comments or by email! Thanks for reading!&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] Duan, Chenru, Du, Yuanqi, Jia, Haojun and Kulik, Heather J. 2023. &lt;em&gt;Accurate transition state generation with an object-aware equivariant elementary reaction diffusion model.&lt;/em&gt; Nature Computational Science. https://dspace.mit.edu/handle/1721.1/153174&lt;/p&gt;

&lt;p&gt;[2] Yun-Fei Shi, Zheng-Xin Yang, Sicong Ma, Pei-Lin Kang, Cheng Shang, P. Hu, Zhi-Pan Liu.
&lt;em&gt;Machine Learning for Chemistry: Basics and Applications&lt;/em&gt;,
Engineering, 2023, ISSN 2095-8099, https://doi.org/10.1016/j.eng.2023.04.013.&lt;/p&gt;

&lt;p&gt;[3] Shi, Zoe. &lt;em&gt;An Introduction to the Chemical Reaction Network Theory&lt;/em&gt;.  https://sites.math.washington.edu/~morrow/336_20/papers20/zoey.pdf&lt;/p&gt;

&lt;p&gt;[4] https://en.wikipedia.org/wiki/Retrosynthetic_analysis&lt;/p&gt;

&lt;p&gt;[5] Victor Garcia Satorras, Emiel Hoogeboom, Max Welling.&lt;em&gt;E(n) Equivariant Graph Neural Networks&lt;/em&gt;. https://arxiv.org/pdf/2102.09844.pdf&lt;/p&gt;

&lt;p&gt;[6] &lt;a href=&quot;https://github.com/chenruduan/OAReactDiff/blob/398ebe052b60e337a04b0a9832549fe5de31c21a/OA-ReactDiff.ipynb&quot;&gt;OA-ReactDiff Github Repo&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;[7] Ståhl N, Falkman G, Karlsson A, Mathiason G, Boström J. &lt;em&gt;Deep Convolutional Neural Networks for the Prediction of Molecular Properties: Challenges and Opportunities Connected to the Data&lt;/em&gt;. J Integr Bioinform. 2018 Dec 5;16(1):20180065. doi: 10.1515/jib-2018-0065. PMID: 30517077; PMCID: PMC6798861.&lt;/p&gt;

&lt;p&gt;[8] J. Chem. Inf. Comput. Sci. 1988, 28, 1, 31–36 Publication Date:February 1, 1988. https://doi.org/10.1021/ci00057a005&lt;/p&gt;

&lt;p&gt;[9] Li Y, Xu Y, Yu Y. &lt;em&gt;CRNNTL: Convolutional Recurrent Neural Network and Transfer Learning for QSAR Modeling in Organic Drug and Material Discovery&lt;/em&gt;. Molecules. 2021; 26(23):7257. https://doi.org/10.3390/molecules26237257&lt;/p&gt;

&lt;p&gt;[10] Senior, A.W., Evans, R., Jumper, J. et al. &lt;em&gt;Improved protein structure prediction using potentials from deep learning&lt;/em&gt;. Nature 577, 706–710 (2020). https://doi.org/10.1038/s41586-019-1923-7&lt;/p&gt;

&lt;p&gt;[11] J. Chem. Inf. Model.2020, 60, 1175−1183. https://pubs.acs.org/doi/10.1021/acs.jcim.9b00943&lt;/p&gt;

&lt;p&gt;[12] Bort W, Baskin II, Gimadiev T, Mukanov A, Nugmanov R, Sidorov P, et al. Discovery of novel chemical reactions by deep generative recurrent neural network. Sci Rep 2021;11(1)&lt;/p&gt;

&lt;p&gt;[13] https://en.wikipedia.org/wiki/Quantitative_structure%E2%80%93activity_relationship&lt;/p&gt;

&lt;p&gt;[14] https://arxiv.org/ftp/arxiv/papers/1812/1812.08434.pdf&lt;/p&gt;

&lt;p&gt;[15] https://arxiv.org/pdf/1712.06113.pdf&lt;/p&gt;

&lt;p&gt;[16] https://arxiv.org/pdf/2108.07258.pdf&lt;/p&gt;

&lt;p&gt;[17] Jumper, J., Evans, R., Pritzel, A. et al. Highly accurate protein structure prediction with AlphaFold. Nature 596, 583–589 (2021). https://doi.org/10.1038/s41586-021-03819-2&lt;/p&gt;

&lt;p&gt;[18] https://arxiv.org/pdf/2310.06083.pdf&lt;/p&gt;

&lt;p&gt;[19] Ucak, U.V., Ashyrmamatov, I., Ko, J. et al. Retrosynthetic reaction pathway prediction through neural machine translation of atomic environments. Nat Commun 13, 1186 (2022). https://doi.org/10.1038/s41467-022-28857-w&lt;/p&gt;

&lt;p&gt;[20] ACS Cent. Sci. 2018, 4, 11, 1465–1476 Publication Date:November 16, 2018. https://doi.org/10.1021/acscentsci.8b00357&lt;/p&gt;

&lt;p&gt;[21] S. Zheng, J. Rao, Z. Zhang, J. Xu, Y. Yang, Predicting retrosynthetic reactions using self-corrected transformer neural networks, J. Chem. Inf. Model. 60 (2020) 47–55, https://doi.org/10.1021/acs.jcim.9b00949&lt;/p&gt;

&lt;p&gt;[22] https://doi.org/10.1016/j.aichem.2024.100049&lt;/p&gt;

&lt;p&gt;[23] https://www.youtube.com/watch?v=ibELoCSf3cE&amp;amp;t=142s&lt;/p&gt;

&lt;p&gt;[24] https://pubs.acs.org/doi/10.1021/acs.accounts.0c00472&lt;/p&gt;

&lt;p&gt;[25] https://www.nature.com/articles/d41586-023-00145-7&lt;/p&gt;

&lt;p&gt;[26] https://pubs.acs.org/doi/pdf/10.1021/acs.est.3c00293&lt;/p&gt;

&lt;p&gt;[27] https://deepblue.lib.umich.edu/bitstream/handle/2027.42/144583/aic16198_am.pdf?sequence=2&lt;/p&gt;

&lt;p&gt;[28] https://pubs.acs.org/doi/epdf/10.1021/acscatal.9b04321&lt;/p&gt;

&lt;p&gt;[29] https://link.springer.com/article/10.1557/s43579-021-00061-9&lt;/p&gt;

&lt;p&gt;[30] https://www.sciencedirect.com/science/article/abs/pii/S0304389422018258&lt;/p&gt;

&lt;p&gt;[31] https://en.wikipedia.org/wiki/High-throughput_screening&lt;/p&gt;

&lt;p&gt;[32] https://en.wikipedia.org/wiki/Omics&lt;/p&gt;

&lt;p&gt;[33] https://pubs.rsc.org/en/content/articlehtml/2022/dd/d1dd00027f&lt;/p&gt;

&lt;p&gt;[34] https://link.springer.com/chapter/10.1007/978-3-030-87013-3_5&lt;/p&gt;

&lt;p&gt;[35] https://tsapps.nist.gov/publication/get_pdf.cfm?pub_id=915933&lt;/p&gt;

&lt;p&gt;[36] https://phys.org/news/2023-09-machine-chemistry-basics-applications.pdf&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Tony Flores</name>
        
        
      </author>

      

      
        <category term="applications" />
      
        <category term="series" />
      
        <category term="literature review" />
      

      
        <summary type="html">Machine Learning in Chemistry</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Machine Learning in Materials Science</title>
      <link href="/polymer-ml" rel="alternate" type="text/html" title="Machine Learning in Materials Science" />
      <published>2022-06-22T08:00:00+00:00</published>
      <updated>2022-06-22T08:00:00+00:00</updated>
      <id>/polymer-ml</id>
      <content type="html" xml:base="/polymer-ml">&lt;h1 id=&quot;machine-learning-in-materials-science&quot;&gt;Machine Learning in Materials Science&lt;/h1&gt;

&lt;p&gt;As the fields of artificial intelligence and machine learning are exploding, their &lt;em&gt;universal&lt;/em&gt; nature is becoming more apparent. Machine learning is being leveraged in a huge variety of sub-fields, and Materials Science and Polymer Science are no exceptions. This article will serve as the first iteration of a series where I’ll briefly discuss research papers related to Machine Learning and Materials Science, in which the goal is simply to explore the different ways that machine learning is being utilized in the field.&lt;/p&gt;

&lt;p&gt;In this iteration, we will focus on polymers. Since Polymer Science is a pretty niche field, I assume you are already familiar with the basics if you are reading this. However, if you are like me, and are just curious by nature, we will preface the paper review with some basics. I must give a disclaimer on my background: although I have worked in a lab where I routinely performed polymer characterization testing, I am in no way an expert– just familiar with the space and some of the lab techniques used. Please keep this in mind as I &lt;strong&gt;attempt&lt;/strong&gt; to touch on the science side of things. :)&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;basics-of-polymers&quot;&gt;Basics of Polymers&lt;/h3&gt;

&lt;p&gt;Before getting into what polymers are on a molecular level, let’s see some familiar materials that are good examples. Some examples of polymers include: plastic, nylon, rubber, wood, protein, and DNA. In this case, we will focus primarily on synthetic polymers like plastic and nylon.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/polyex.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;At the molecular level, polymers are composed of long chains of repeating molecules. The molecule that repeats in this chain is known as a monomer (or subunit). Monomers are like the links in a chain. The image below illustrates multiple monomers being chained together to create a polymer, where the repeating sequence (denoted in brackets) can repeat n- times. 
&lt;img src=&quot;/assets/images/polyethylene.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There is a &lt;strong&gt;huge&lt;/strong&gt; amount of potential molecules that can be used as monomers: the paper we will look at mentions that around 10&lt;sup&gt;8&lt;/sup&gt; compounds are known, while it is believed that up to 10&lt;sup&gt;60&lt;/sup&gt; could exist! Let’s check out some classic monomer examples.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/monomers.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Clearly, monomers come in all shapes and sizes! This is important, because the monomer is the basic building block of the material, so a different molecular structure can result in drastically different material properties. Molecular design (design and discovery of new molecules to use as monomers) is a key area of research in polymer science, and we will see some papers that focus on this (in this post, and in future iterations).&lt;/p&gt;

&lt;p&gt;Polymer chains don’t always have to consist of just one type of repeating unit. A &lt;strong&gt;homopolymer&lt;/strong&gt; is a polymer where the repeating subunit is the same, whereas a &lt;strong&gt;copolymer&lt;/strong&gt; will contain two types of subunits. There are even terpolymers (3) and tetrapolymers (4)! When polymers have more than one type of subunit, they don’t necessarily have to alternate, as you can see in the illustration below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/copolymer.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Different compositions and configurations of subunits can yield different properties. On top of the primary structure (linear sequence of the chain), polymers can also contain branched chains, cross-links, or network structures.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/branchedpoly.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/crosspoly.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/netpoly.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Structural orientation and morphology on a higher order will affect properties as well. Further details are beyond the scope of this post, but the point to take home here is there is a lot of complexity to take into account, and on top of that, there are a seemingly infinite amount of possibilities in structure and composition (all which will change the properties and behavior of the material). With that said, it’s clear that it would be essentially impossible to explore all the possibilities in the lab considering time, cost of materials and labor, etc. This is where computation and machine learning come in!&lt;/p&gt;

&lt;p&gt;In polymer informatics, machine learning can be applied to accelerated characterization of polymers, modeling and analysis, prediction of properties, and molecular design. Applying ML to the research workflow can lead to substantial savings in time and money, since most of the trial-and-error process is reduced by computation and prediction, greatly narrowing the search space for the actual work done in the lab. It sounds like an incredible solution to advancing research, but there are key challenges that must be taken into consideration:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;limited data related to polymers&lt;/li&gt;
  &lt;li&gt;normally requires a hybrid approach with expert input&lt;/li&gt;
  &lt;li&gt;lab analysis required for validation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As more work is done in this space, the impact of these challenges should reduce significantly. With that said, the paper chosen for this review focuses on molecular design, and has a really interesting way of overcoming one of these key challenges. Let’s check it out!&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;paper-review-machine-learning-assisted-discovery-of-polymers-with-high-thermal-conductivity-using-a-molecular-design-algorithm&quot;&gt;Paper Review: Machine-learning-assisted discovery of polymers with high thermal conductivity using a molecular design algorithm&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;Wu, S., Kondo, Y., Kakimoto, Ma. et al. &lt;a href=&quot;https://rdcu.be/cNgzd&quot;&gt;Machine-learning-assisted discovery of polymers with high thermal conductivity using a molecular design algorithm&lt;/a&gt;. npj Comput Mater 5, 66 (2019). https://doi.org/10.1038/s41524-019-0203-2&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Molecular design with machine learning is extremely promising, and there is a lot of work being done exploring this space. In this paper, the objective was to design new polymer material (focusing on the chemical structure of the repeating subunit) with a high &lt;a href=&quot;https://www.sciencedirect.com/topics/materials-science/thermal-conductivity&quot;&gt;thermal conductivity&lt;/a&gt;, on-par with state-of-the-art &lt;a href=&quot;https://en.wikipedia.org/wiki/Thermoplastic&quot;&gt;thermoplastics&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/thermo.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;What makes this paper interesting is the fact that the molecular structures proposed by the model were synthesized and &lt;em&gt;verified in the lab&lt;/em&gt;. To quote the authors:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;“In the particular case of polymers, it is unprecedented that designed polymers were synthesized and experimentally confirmed.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Another fascinating aspect of this work is the use of domain knowledge to overcome a dataset with only 28 training instances with thermal conductivity data. The authors used alternate data, and instead targeted &lt;a href=&quot;https://www.sciencedirect.com/topics/chemistry/glass-transition-temperature&quot;&gt;glass transition temperature&lt;/a&gt; and melting point as “proxy properties”, since they know polymers with higher glass transition and melting temperatures tend to also have higher thermal conductivity.&lt;/p&gt;

&lt;h3 id=&quot;bayesian-molecular-design-using-probabilistic-language-models&quot;&gt;Bayesian Molecular Design using Probabilistic Language Models&lt;/h3&gt;

&lt;p&gt;Typically, when it comes to molecular design with machine learning, prediction runs in two directions. The “forward” direction is known as the &lt;strong&gt;quantitative structure-property relationship&lt;/strong&gt;, or &lt;strong&gt;QSPR&lt;/strong&gt;. Essentially in this direction, &lt;em&gt;physical and chemical properties are predicted&lt;/em&gt; with a given chemical structure input. The backward direction is known as &lt;strong&gt;inverse-QSPR&lt;/strong&gt;, and is exactly what it sounds like: given a set of target physical or chemical properties, a &lt;em&gt;molecular structure is predicted&lt;/em&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Sidenote: Quantitative structure-activity relationship (QSAR) is similar to QSPR, but instead models the relationship between molecular structure and &lt;strong&gt;biological activity&lt;/strong&gt;, instead of physical/chemical properties. This type of molecular design is essential for drug discovery.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In this paper, the machine learning algorithm known as Bayesian molecular design was used. As stated before, the objective was to design the molecular structure of the monomer unit given a set of target physical properties (iQSPR). The chemical structures are represented using simplified molecular-input line-entry system (SMILES) notation, which essentially compresses the chemical structure into a string:&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;&lt;img src=&quot;../assets/images/SMILES.png&quot; alt=&quot;&quot; /&gt;&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;&lt;em&gt;Visualization of the SMILES algorithm. Image Source: &lt;a href=&quot;https://en.wikipedia.org/wiki/Simplified_molecular-input_line-entry_system&quot;&gt;Wikipedia&lt;/a&gt;&lt;/em&gt;&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The forward model is defined as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p(Y∈U|S)&lt;/code&gt;, and the backward model defined as &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;p(S|Y∈U)&lt;/code&gt;:&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;the probability of a chemical structure &lt;strong&gt;S&lt;/strong&gt; given that it’s physical property &lt;strong&gt;Y&lt;/strong&gt; lies in the region &lt;strong&gt;U&lt;/strong&gt;, where &lt;strong&gt;Y&lt;/strong&gt; is thermal conductivity and &lt;strong&gt;U&lt;/strong&gt; is the desired range of values.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;prior-distribution&quot;&gt;Prior Distribution&lt;/h3&gt;
&lt;p&gt;A prior distribution &lt;strong&gt;p(S)&lt;/strong&gt; was used to minimize the occurrence of invalid chemical structures predicted by the model. To define &lt;strong&gt;p(S)&lt;/strong&gt;, an &lt;a href=&quot;https://web.stanford.edu/~jurafsky/slp3/3.pdf&quot;&gt;n-gram model&lt;/a&gt; of order &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;n = 10&lt;/code&gt; was trained on 14,423 homopolymers in the &lt;a href=&quot;https://polymer.nims.go.jp/en/&quot;&gt;&lt;strong&gt;PolyInfo&lt;/strong&gt;&lt;/a&gt; database to learn realistic and favorable structural molecular patterns.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;../assets/images/distribution.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;forward-prediction-step&quot;&gt;Forward Prediction Step&lt;/h3&gt;
&lt;p&gt;1000 neural nets were trained using the forward prediction model, aiming to predict &lt;strong&gt;glass transition temp.&lt;/strong&gt; and &lt;strong&gt;melting temp.&lt;/strong&gt; with the monomer chemical structure as input. From here, the model that performed the best at predicting the original target of &lt;strong&gt;thermal conductivity&lt;/strong&gt; was used, and fine-tuned on the small amount of &lt;em&gt;thermal conductivity&lt;/em&gt; data using transfer learning and &lt;a href=&quot;https://github.com/apache/incubator-mxnet&quot;&gt;&lt;strong&gt;MXNet&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;backward-prediction-step-and-validation&quot;&gt;Backward Prediction Step and Validation&lt;/h3&gt;
&lt;p&gt;Finally, with the prior distribution and the forward model, the backward model was formed and used to generate 1000 chemical structures using a sequential Monte Carlo method. From these structures, 3 were selected based off of their theoretical synthesizability and ease of processing using an &lt;a href=&quot;https://www.researchgate.net/publication/42344111_Estimation_of_Synthetic_Accessibility_Score_of_Drug-Like_Molecules_Based_on_Molecular_Complexity_and_Fragment_Contributions&quot;&gt;SA Score&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The 3 selected polymers (referred to as 4, 13, and 19) were synthesized, and their thermophysical properties were tested in the lab. The thermal conductivity absolute prediction errors for 4, 13, and 19 were 0.015, 0.001, and 0.017 W/mK, respectively, indicating a high level of agreement between predicted and experimental values. In the end, the values ranged from &lt;strong&gt;0.18-0.41 W/mK&lt;/strong&gt;, which are comparable to state-of-the-art thermoplastics, and aligned with the range the researchers were targeting.&lt;/p&gt;

&lt;p&gt;These results are really exciting to see, especially considering that openly available data in this space is still lacking. If the scientific community can contribute and significantly expand available polymer data, I think some really promising research can be done utilizing machine learning. If you are curious about the details of the methods used in this paper, &lt;a href=&quot;https://www.nature.com/articles/s41524-019-0203-2.epdf?sharing_token=E-iwfNr-YoElwGuzhmRbBtRgN0jAjWel9jnR3ZoTv0MHuyid8yZr0Vl4erv1AGAsyqMODXQ5YqlX4Z5s71TTgogZgQlqZgpFdZrBqNU0nPODTOhlTzOsLYIIZFpB_jKQCoUgtTuuZM4fYqpUlTCDMvlrzATIFGGMjVg4CtDfgGY%3D&quot;&gt;I encourage you to give it a read&lt;/a&gt;!&lt;/p&gt;

&lt;h1 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;It’s really fascinating to see machine learning applied to different domains, and I think some really cool and important results can be achieved in the materials science and engineering space, especially digging into optimizing for sustainability and improving environmental friendliness. :) 
I hope to continue this series for a long time, and welcome any criticism or suggestions for improvement! Thanks for reading!&lt;/p&gt;

&lt;h1 id=&quot;references&quot;&gt;References&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://link.springer.com/article/10.1007/s10822-016-0008-z&quot;&gt;https://link.springer.com/article/10.1007/s10822-016-0008-z&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7050509/&quot;&gt;https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7050509/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://onlinelibrary.wiley.com/doi/10.1002/inf2.12167&quot;&gt;https://onlinelibrary.wiley.com/doi/10.1002/inf2.12167&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Tony Flores</name>
        
        
      </author>

      

      
        <category term="applications" />
      
        <category term="literature review" />
      
        <category term="series" />
      
        <category term="machine learning" />
      

      
        <summary type="html">Machine Learning in Materials Science</summary>
      

      
      
    </entry>
  
</feed>
